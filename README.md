# Distribution of energy sources & temperatures over time in France.

![ecomix](medias/eco2mix.jpeg) 
<a href="https://www.freepik.com/free-vector/generation-energy-types-power-plant-icons-vector-set-renewable-alternative-solar-tidal-wind-geotermal-biomass-wave-illustration_10601053.htm#track=ais">Image by macrovector</a> on Freepik

---
### Table of contents
- [Objective](#objective)
- [Data sources](#data-sources)
- [Tools](#tools)
- [Reproducing this project](#reproducing-this-project)

---
## Objective

This data engineering project aims to deploy and regularly update a data pipeline from end to end. To do so, this project will use the following steps:

1. Copy data **from the data-sources to the data-lake** *(Google Cloud Storage in this case)*
2. Copy data **from the data-lake to the data-warehouse** *(BigQuery in this case)*
3. Transform and **produce interesting data from the staging-data**, and store them in developpement/production-data *(using DBT)*
4. Create a **custom dashboard to display some of the information** stored in the poduction-data.

![ecomix](medias/data-pipeline.png)

--- 
## Data sources

In order to fullfil this project, I selected **3 datasets from the ODRÉ** (Open Data Réseaux Energies) website.

Their dataset are particularly interesting to me, because they provide **live data** that comes diretly **from energy carriers and their partners**, and also because the dataset are **refreshed at various frequencies** ( every 1 hour / 1 day / 1 month).

Furthermore, I think we can use those datasets to answers **interesting questions** such as:
1. What is the repartition of energy sources for a given day or a given period?
2. Is the gas stock correlated in any way with the temperature?
3. Are the energy sources influenced by the temperature?
4. Are the energy sources influenced by the current gas stock?
5. Does the energy sources repartition change with seasons?
6. Are there more commercial exchanges related to energy when the gas stock is full at the borders?
7. ... 



### Dataset 01: `eco2mix-national-tr`

> https://odre.opendatasoft.com/explore/dataset/eco2mix-national-tr

This dataset, refreshed once an hour, presents "real time" data from the éCO2mix application. They come from the telemetry of the infrastructures and are completed with estimations.

It contains:
- The actual consumption.
- The consumption forecasts established the day before (D-1) and those updated the same day (D).
- The production according to the different sectors of the energy mix.
- The consumption of pumps in Pumped Storage Facilities (PST).
- Physical exchanges at the borders.
- An estimate of the carbon emissions generated by electricity production in France.
- The breakdown of the production mix into sectors and technologies.
- The commercial exchanges at the borders.

### Dataset 02: `stock-quotidien-stockages-gaz`

> https://odre.opendatasoft.com/explore/dataset/stock-quotidien-stockages-gaz

This dataset presents the gas stock present in the Teréga and Storengy gas storage facilities, at the end of each day and by PITS since November 1, 2010 (GWh PCS 0°C).

### Dataset 03: `temperature-quotidienne-regionale`

> https://odre.opendatasoft.com/explore/dataset/temperature-quotidienne-regionale

This dataset presents the daily minimum, maximum and average temperatures (in degrees Celsius), by French administrative region, from January 1, 2016 to today. It is based on the official measurements of the French weather station network. The update of this dataset is monthly.

---
## Tools

- **Terraform**: to easily provision the required infrastructures.
- **Prefect**: to execute python scripts that transfer the datasets.
- **dbt**: to transform the data and create new tables from the original datasets.
- **Google Cloud Storage**: for the Data-lake.
- **Google BigQuery**: for the Data-warehouse.
- **Google Looker Studio**: for the Data-visualization.

- **Python**: to write the various *Prefect* scripts.
- **SQL**: to write the various *dbt* models

---
## Reproducing this project

### 1. Setup 

Install PREFECT and other libs (starting at the root folder of the project)

```bash
>>> python -m venv venv
>>> source venv/bin/activate
>>> pip install -r requirements.txt
>>> prefect version
```

### 2. Initialize infrastuctures with Terraform

```bash
(venv) >>> terraform -chdir=terraform plan # (optional) 
(venv) >>> terraform -chdir=terraform apply # (answer 'yes')
```

> At this point you should see:
> - a new empty **eco2mix-de-project-bucket** in your GCP bucket list.
> - three new empty tables in your BigQuery database:
>     - **de_project_staging** (the source tables for DBT)
>     - **de_project_development** (the DBT tests exports)
>     - **de_project_production** (the DBT final exports)

### 3. Initialize Prefect Orion

if you want to run Prefect from Cloud
```bash
>>> TODO
```

And if you prefer to run it locally
```bash
(venv) >>> prefect orion start
```
open http://127.0.0.1:4200/

![ecomix](medias/cmd_prefect_orion.png)

### 3. Initialize Prefect Workflow

Edit the MakeFile with your *project_id* and the *path to your GCP credential json file*.

```bash
(venv) >>> make setup
```

<details>
  <summary>If you can't use *make* for any reason click here</summary>
  
  > The make file basically execute the following lines
  > ```bash
  > (venv) >>> python ../setup_prefect.py {CREDS_PATH} {PROJECT_ID}
  > (venv) >>> prefect deployment build flows/etl_main.py:etl_main_flow --name etl_eco2mix --cron "0 * * * *" -a
  > ```
</details>


> At this point the Prefect UI should deplay:
> - one deployement **etl-main-flow/etl_eco2mix**
>
> ![ecomix](medias/result_prefect_deployment.png)
> - three blocks
>     - GCP Credentials / **eco2mix-de-project-creds**
>     - GCS Bucket / **eco2mix-de-project-bucket**
>     - JSON / **eco2mix-de-project-variables**
>
> ![ecomix](medias/result_prefect_blocks.png)

### 5. Start Prefect Agent

```bash
(venv) >>> prefect agent start --work-queue "default"
```

![ecomix](medias/cmd_prefect_agent.png)

and if you don't want to wait for the cronjob time, run the following in a new terminal:
```bash
(venv) >>> prefect deployment run etl-main-flow/etl_eco2mix
```

### 6. Setup DBT

TODO

> At this point, the BigQuery **de_project_production** table should be filled with a *partitioned* **daily_merged** table.
>
> ![ecomix](medias/result_dbt.jpg)

### 7. Setup Looker

Using the *BigQuery / daily_merged / Export / Explore with Looker studio*, one can finally build a new dashboard.

> Here is the dashbord I created for this project
> ![ecomix](medias/looker.jpg)
> It can be tested here: https://lookerstudio.google.com/s/iaO82EDWadE

### 8. Clean up

Once done, don't forget to remove the allocated infrastructures, clean the ressources etc...

#### Destroy infrastuctures with Terraform

```bash
(venv) >>> terraform -chdir=terraform destroy # (answer 'yes')
```
This will remove the GCP Bucket and the three tables from the BigQuery database.

#### Clean Prefect

If you don't intend to reuse the Prefect blocks and deployment, remove them from your Prefect Orion (either local or in cloud). Then stop the prefect orion and prefect agent with CTRL+C.

#### Remove the virtual environment

```bash
(venv) >>> deactivate
>>> rm -r venv
```