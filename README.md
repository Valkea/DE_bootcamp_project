# Distribution of energy sources & temperatures over time in France.

![ecomix](medias/eco2mix.jpeg) 
<a href="https://www.freepik.com/free-vector/generation-energy-types-power-plant-icons-vector-set-renewable-alternative-solar-tidal-wind-geotermal-biomass-wave-illustration_10601053.htm#track=ais">Image by macrovector</a> on Freepik

---
### Table of contents
- [Objective](#objective)
- [Data sources](#data-sources)
- [Tools](#tools)
- [Reproducing this project](#reproducing-this-project)

---
## Objective

This data engineering project aims to deploy and regularly update a data pipeline from end to end. To do so, this project will use the following steps:

1. Copy data **from the data-sources to the data-lake** *(Google Cloud Storage in this case)*
2. Copy data **from the data-lake to the data-warehouse** *(BigQuery in this case)*
3. Transform and **produce interesting data from the staging-data**, and store them in developpement/production-data *(using DBT)*
4. Create a **custom dashboard to display some of the information** stored in the poduction-data.

![ecomix](medias/data-pipeline.png)

--- 
## Data sources

In order to fullfil this project, I selected **3 datasets from the ODRÉ** (Open Data Réseaux Energies) website.

Their dataset are particularly interesting to me, because they provide **live data** that comes diretly **from energy carriers and their partners**, and also because the dataset are **refreshed at various frequencies** ( every 1 hour / 1 day / 1 month).

Furthermore, I think we can use those datasets to answers **interesting questions** such as:
1. What is the repartition of energy sources for a given day or a given period?
2. Is the gas stock correlated in any way with the temperature?
3. Are the energy sources influenced by the temperature?
4. Are the energy sources influenced by the current gas stock?
5. Does the energy sources repartition change with seasons?
6. Are there more commercial exchanges related to energy when the gas stock is full at the borders?
7. ... 



### Dataset 01: `eco2mix-national-tr`

> https://odre.opendatasoft.com/explore/dataset/eco2mix-national-tr

This dataset, refreshed once an hour, presents "real time" data from the éCO2mix application. They come from the telemetry of the infrastructures and are completed with estimations.

It contains:
- The actual consumption.
- The consumption forecasts established the day before (D-1) and those updated the same day (D).
- The production according to the different sectors of the energy mix.
- The consumption of pumps in Pumped Storage Facilities (PST).
- Physical exchanges at the borders.
- An estimate of the carbon emissions generated by electricity production in France.
- The breakdown of the production mix into sectors and technologies.
- The commercial exchanges at the borders.

### Dataset 02: `stock-quotidien-stockages-gaz`

> https://odre.opendatasoft.com/explore/dataset/stock-quotidien-stockages-gaz

This dataset presents the gas stock present in the Teréga and Storengy gas storage facilities, at the end of each day and by PITS since November 1, 2010 (GWh PCS 0°C).

### Dataset 03: `temperature-quotidienne-regionale`

> https://odre.opendatasoft.com/explore/dataset/temperature-quotidienne-regionale

This dataset presents the daily minimum, maximum and average temperatures (in degrees Celsius), by French administrative region, from January 1, 2016 to today. It is based on the official measurements of the French weather station network. The update of this dataset is monthly.

---
## Tools

- **Terraform**: to easily provision the required infrastructures.
- **Prefect**: to execute python scripts that transfer the datasets.
- **dbt**: to transform the data and create new tables from the original datasets.
- **Google Cloud Storage**: for the Data-lake.
- **Google BigQuery**: for the Data-warehouse.
- **Google Looker Studio**: for the Data-visualization.

- **Python**: to write the various *Prefect* scripts.
- **SQL**: to write the various *dbt* models.
- **Makefile**: to simpify the deployment processus.

---
## Reproducing this project

### 1. Setup GCP

TODO

### 2. Setup local environment

Let's duplicate the project github repository

```bash
>>> git clone https://github.com/Valkea/DE_bootcamp_project.git
>>> cd DE_bootcamp_project
```

Then, let's install PREFECT and other libs (starting at the root folder of the project)

```bash
>>> python -m venv venv
>>> source venv/bin/activate
>>> pip install -r requirements.txt
>>> prefect version # (just to check that prefect is installed)
```

### 3. Initialize infrastuctures with Terraform

```bash
(venv) >>> terraform -chdir=terraform plan # (optional) 
(venv) >>> terraform -chdir=terraform apply # (answer 'yes')
```

> At this point you should see:
> - a new empty **eco2mix-de-project-bucket** in your GCP bucket list.
> - three new empty tables in your BigQuery database:
>     - **de_project_staging** (the source tables for DBT)
>     - **de_project_development** (the DBT tests exports)
>     - **de_project_production** (the DBT final exports)
>
> ![ecomix](medias/result_terraform_apply.png)

### 4. Initialize Prefect Orion in Cloud or Locally


<details>
  <summary>(option 1): to use Prefect Orion in the Cloud (recommended), click here</summary>
  
  > 1. Create a [PREFECT cloud account](http://prefect.io/)
  > 2. Login from local terminal 
  > ```bash
  > (venv) >>> prefect cloud login
  > ```
  >
  > ![ecomix](medias/cmd_prefect_cloud.png)
  >
  > 3. (optional) Changing the workspace
  > ```bash
  > (venv) >>> prefect cloud workspace set --workspace "my_workspace_sname"
  > ```

</details>

<details>
  <summary>(option 2): to use Prefect Orion locally, click here</summary>
  
  > The make file basically execute the following lines
  > ```bash
  > (venv) >>> prefect orion start
  > ```
  >
  > ![ecomix](medias/cmd_prefect_orion.png)
  >
  > open http://127.0.0.1:4200/
</details>


### 5. Initialize Prefect Workflow

**Edit the MakeFile** with your *project_id* and the *path to your GCP credential json file*, and initialize using the folloing command

```bash
(venv) >>> make setup
```

<details>
  <summary>If you can't use *make* for any reason click here</summary>
  
  > The make file basically execute the following lines
  > ```bash
  > (venv) >>> python setup_prefect.py {CREDS_PATH} {PROJECT_ID}
  > (venv) >>> prefect deployment build flows/etl_main.py:etl_main_flow --name etl_eco2mix --cron "0 * * * *" -a
  > ```
</details>


> At this point the Prefect Orion UI *(either in the Cloud or Locally)* should deplay:
> - one deployement **etl-main-flow/etl_eco2mix**
>
> ![ecomix](medias/result_prefect_deployment.png)
> - three blocks
>     - GCP Credentials / **eco2mix-de-project-creds**
>     - GCS Bucket / **eco2mix-de-project-bucket**
>     - JSON / **eco2mix-de-project-variables**
>
> ![ecomix](medias/result_prefect_blocks.png)

### 6. Start Prefect Agent

Whether we use Prefect Orion in the Cloud or Locally, we need to initialize a *Prefect agent* in order to execute the selected queue

```bash
(venv) >>> prefect deployment run etl-main-flow/etl_eco2mix # (to avoid waiting for the cronjob time)
(venv) >>> prefect agent start --work-queue "default"
```

> ![ecomix](medias/cmd_prefect_agent.png)


### 7. Setup DBT

TODO

> At this point, the BigQuery **de_project_production** table should be filled with a *partitioned* **daily_merged** table.
>
> ![ecomix](medias/result_dbt.jpg)

### 8. Setup Looker

Using the *BigQuery / daily_merged / Export / Explore with Looker studio*, one can finally build a new dashboard.

> Here is the dashbord I created for this project
>
> ![ecomix](medias/looker.jpg)
>
> It can be tested here: https://lookerstudio.google.com/reporting/a0869f4e-7f24-4b50-8dd0-750aac025e3b

### 9. Clean up

Once done, don't forget to remove the allocated infrastructures, clean the ressources etc...

#### Destroy infrastuctures with Terraform

```bash
(venv) >>> terraform -chdir=terraform destroy # (answer 'yes')
```
This will remove the GCP Bucket and the three tables from the BigQuery database.

#### Clean Prefect

If you don't intend to reuse the Prefect blocks and deployment, remove them from your Prefect Orion (either local or in cloud). 
Then logout from the cloud is using the Cloud:
```bash
(venv) >>> prefect cloud logout
```
And finally stop the *Prefect agent* with CTRL+C

#### Remove the virtual environment

```bash
(venv) >>> deactivate
>>> rm -r venv
```